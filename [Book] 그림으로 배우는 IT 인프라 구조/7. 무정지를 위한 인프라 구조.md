### 안정성 및 이중화

상용 시스템을 장애로부터 보호하기 위해서 빠트릴 수 없는 구조가 바로 안정성과 이중화다.

상용 시스템에서는 인프라가 '업무 기능의 토대'가 되지만 요구되는 몇 가지가 있다. 그 중 하나가 안정성이다. 고가용성이라고도 한다.

안정성, 고가용성이란, 시스템 서비스가 가능한 한 멈추지 않도록 하는 것을 말한다.

안정성, 고가용성의 목표

1. 고장, 장애에 의한 정지가 발생하지 않을 것
2. 고장, 장애가 발생해도 복구할 수 있을 것
3. 고장, 장애가 발생한 것을 검출할 수 있을 것
4. 고장, 장애가 발생해도 데이터가 보호될 것

컴포넌트 이중화: 1, 2
컴포넌트 감시: 3
데이터 백업: 4

상용 웹 시스템에서는 미들웨어 기능이나 구조로 이중화, 감시, 백업의 세 가지 수단을 구현해서 목표를 실현하고 있다. 또한, 이중화에는 안정성 확보 이외에도 다른 용도가 있다.

#### 이중화란?

이중화는 하나의 기능을 병렬로 여러 개 나열해서 하나에 장애가 발생해도 다른 것을 이용해서 서비스를 계속할 수 있는 것을 가리킨다. 하나의 기능이 병렬로 가동되기 때문에 이런 고가용성에 대한 의미뿐만 아니라 확장성이나 부하 분산 같은 성능에 대한 의미도 가진다.

이중화에 의한 안정성은 다양한 계층에서 실현할 수 있다.

1. H/W 컴포넌트 계층

- 전원이나 네트워크, 하드디스크 이중화다. 신뢰도가 비교적 낮은 부분과 데이터 보호를 목적으로 한다.

2. 시스템 계층

- 여러 서버에서 동일한 처리를 하는 서버 이중화다. 특정 서버의 처리가 중단돼도 다른 서버에서 처리를 지속해서 사용자에게는 영향을 주지 않는 것이 목적이다.

3. 물리적 위치 계층

- 시스템을 서로 다른 위치에서 운영하는 이중화다. 비용이 들지만 중요한 시스템을 위해 많은 기업들이 채택하는 방식이다. 비용과 신뢰도는 상호 상호의존관계다. 얼마나 비용을 아껴서 시스템의 신뢰도를 높이는가가 관건이다.

### 서버 내 이중화

일반적인 서버 내부 컴포넌트에서는 전원, 팬 등이 이중화돼 있다. 이들 중 하나가 망가져도 서버가 정상 가동하도록 설계돼 있다.

랙 뒤쪽의 양 끝에는 전원 탭이 붙어 있다. 양 끝에 있는 이유는 이중화 때문이다.
서버 설치 시에 이와 같이 설치해 두면 장애를 예방할 수 있다. 또한, 대규모 데이터 센터에서는 각 전원 탭이 별도 분전반이나 UPS에 접속돼 있어서 전원 장애에 대비하고 있다.

안정성을 고려할 때 중요한 것은 양쪽 전원 탭의 전력 합계를 최대로 사용하는 것이 아니라, 한쪽 전원 탭 전력만으로 서버가 가동될 수 있도록 소비 전력 합계를 낮추는 것이다. 한쪽 전원이 모두 공급되지 않아도 다른 한쪽의 전원만으로 시스템을 가동할 수 있기 때문이다. 서버의 소비 전력은 서버 사양서에 기록돼 있다.

#### 네트워크 인터페이스 이중화

PCI 슬롯에 꽂은 카드도 이중화가 가능하다. 네트워크 인터페이스나 파이버 채널 포트에는 이중화 기능이나 이중화를 실현하는 소프트웨어가 있다. 이 기능들을 전제로 카드 이중화 및 포트 이중화를 할 수 있다. 여러 개의 카드에 복수의 포트가 탑재되는 것이다. 이런 구성을 통해 카드 장애 및 포트 장애에 대응할 수 있다.
네트워크 인터페이스가 여러 개 있으면 이중화 소프트웨어나 기능을 이용해서 가용성과 성능을 향상시킬 수 있다.

네트워크 인터페이스 이중화는 하드웨어 또는 OS로 구현한다. 일반적으로는 액티브-스탠바이 구성이다. 액티브-스탠바이 구성은 마스터-워커 개념에 기반했다. 스탠바이 측은 보통 서비스를 제공하지 않는다. 액티브 측에 어떤 문제가 발생하면 스탠바이 장비로 교체돼서 스탠바이가 액티브로 변경된다. 이렇게 교체하는 것을 페일오버라고 한다.

네트워크 인터페이스 이중화의 대표적인 구현 방법 중 하나는 리눅스 OS의 본딩이다. 본딩에는 몇 가지 모드가 있는데, 그중 하나인 액티브-스탠바이 구성은 '액티브-백업'이라고 한다. 어떤 인터페이스가 마스터가 되는지는 설정에서 지정할 수 있다.

본딩이 이중화된 인터페이스를 감시하는 방식에는 두 가지가 있다. 'MII 감시'와 'ARP 감시'다. MII 감시는 MII(Media Independent Interface) 규격에 준거한 인터페이스의 링크 감시로, 현재 주류로 자리 잡고 있는 감시 방식이다. MII 감시에는 링크업(인터페이스가 가동되는 것)이 동작하고 있다면 정상이라고 판단한다. 한편, ARP 감시는 특정 IP 주소로 ARP 요청을 보내서 돌아오는 응답 유무에 따라 정상적인지를 확인하는 방법이다.

MII 감시가 선호되는 주된 이유는 다음과 같다:

- 불필요한 폴링 패킷이 전송되지 않는다.
- 폴링 위치로 지정한 IP 주소를 가진 장비에 대해서는 유지관리나 장애를 의식하지 않아도 된다.

물론 ARP 감시로만 인지할 수 있는 장애도 있다.

ARP 감시는 L2 계층에서 실시된다. ARP 요청을 정기적으로 실행해서 응답이 있으면 정상이라 판단한다. ARP 요청은 MAC 주소를 확인하는 브로드캐스트다. (브로드캐스트는 동일 네트워크 내의 모든 주소에 패킷을 전송하는 것)

ARP 감시의 단점은 폴링이 되는 ARP 요청이 동일 네트워크 내의 브로드캐스트이기 때문에 불필요한 트래픽이 증가한다는 것이다. 따라서 이 불필요한 트래픽을 고려해서 감시 빈도를 너무 높게 설정하지 않는 것이 중요하다. ARP 감시는 일반적으로 1~2초 간격 정도로 설정한다. 페일오버 구조는 감시 방법에 상관없이 동일하다.

인터페이스가 페일오버하면 스위치의 MAC 주소 테이블이 변경되고 통신이 재개된다. 이와 같이 네트워크 인터페이스 이중화인 본딩의 구현은 L3 네트워크보다 낮은 게층에서 구현되고 있다. 때문에 이중화 기능이 L7 애플리케이션 계층에서 구현돼 있으면 이상하다고 판단할 수 있는 감각이 필요하다.

ARP 감시에서는 임의의 IP 주소에 대해 ARP 요청을 보낼 수 있다. 주로 네트워크 게이트웨이로 요청을 보낸다. 반면 MII 감시는 인터페이스 링크 상태를 확인하기 때문에 접속돼 있는 L2스위치만 감시할 수 있다.

ARP 감시가 더 넓은 범위를 감시한다는 것을 알 수 있다. 이와 같이 기능의 장단점과 특성을 고려해서 시스템을 구현해야 한다.

### 저장소 이중화

#### HDD 이중화

저장소 이중화의 주요 대상은 HDD다. HDD는 가동 빈도가 높아서 고장 나기 쉽기 때문이다.

최근에는 잘 사용되지 않지만, 예전에는 서버 저장소를 파이버 채널로 연결하고 SAN(Storage Area Network)이라는 네트워크를 구축하는 방법이 유행했다. 하지만 최근에 접할 수 없는 이유는 높은 비용과 변경(증설 등)에 시간이 걸리기 때문이다.

요즘엔 TCP/IP 프로토콜상에 저장소 네트워크를 구축하는 방식이 늘고 있다. 하지만 TCP/IP 프로토콜상에 SCSI 프로토콜을 이용하는 등, 여러 기술을 조합했기 때문에 사용은 쉽지만 구조가 그만큼 복잡하다. 반면 SAN은 구조가 간단해서 이를 전제로 한 기술이 다시 개발되고 있다.

##### 저장소 내부 구조와 RAID

SAN에서는 IP 주소 대신에 WWN(World Wide Name)이라는 주소를 이용해서 데이터를 전송한다. LAN과는 다른 네트워크 토폴로지다. 또한, HDD 간을 연결하는 내부 버스에는 다양한 규격이 있지만 SAS(Serial Attached SCSI)가 유명하다.

컨트롤러에는 CPU나 캐시가 있으며, HDD의 I/O 제어를 하고 있다. HDD는 전용 박스(엔클로저나 셸프라고 한다)에 저장돼 있어서 증설하기가 쉽다.

각 장비에 IN과 OUT 포트가 있어서 계속 따라가다 보면 HDD까지 원형 연결이 만들어지는 것을 알 수 있다. 저장소는 이 원형 연결을 여러 개 준비해서 HDD 액세스 이중화를 도모하고 있다.

참고로, 서버가 저장소에 액세스할 때는 일반적인 액티브-스탠바이 또는 액티브-액티브 방식을 이용한다.

HDD 자체 이중화는 RAID를 사용한다.
RAID는 여러 HDD를 묶어서 그룹으로 만들고 이것을 논리적인 HDD로 인식하는 기술이다. 논리적 HDD를 LU(Local Unit)라고 한다. 서버가 인식하는 HDD는 이 LU다.

RAID의 장점은 다음과 같다.

1. 안정성 확보
   RAID에서는 HDD에 장애가 발생해도 데이터가 손실되지 않도록 데이터 기록을 이중화한다. HDD는 움직이는 부분이 많아서 고장이 쉽게 난다. 따라서 기록 처리를 이중화한다는 것은 매우 중요한 의미를 가진다.
2. 성능 향상
   RAID에서는 RAID 컨트롤러가 미리 정해놓은 길이로 I/O를 분할해서 복수의 HDD에 대해 병렬로 I/O 처리를 한다. 이 고정 길이를 RAID의 스트라이프 크기라고 부른다.

여러 개의 HDD를 병렬로 동시에 동작시키기 때문에 하나의 HDD를 동작시킬 때보다 I/O 처리 성능이 높아진다. 이 특성을 살리려면 하나의 RAID 그룹에 포함하는 HDD 수를 늘리면 된다. 대신에 한 대의 HDD 고장이 끼치는 영향 범위도 커지기 때문에 상충 관계에 있다고 할 수 있다. 최근에는 I/O 성능 중시 경향이 있어서 하나의 RAID 그룹 크기가 커지고 있다. 8 ~ 15대 정도가 하나의 그룹을 이룬다.

3. 용량 확장
   한 대의 HDD 용량은 기술 발전과 함께 점점 커지고 있지만, 그래도 물리적으로는 600GB, 1TB 등으로 정해져 있다. 논리 HDD는 이런 물리적 한계를 넘어서 자유롭게 용량을 결정할 수 있다. 예를 들어, 10TB를 하나의 파일 시스템으로 취급할 수도 있다.

RAID 구성 패턴

RAID에는 몇 가지 구성 패턴이 존재한다. RAID1, RAID5, RAID10과 같은 구성 패턴이 주류다.

RAID5는 이중화 확보를 위해 패리티라는 오류 수정 부호를 기록한다. 패리티를 하나의 HDD에 집중시키지 않고 분산하는 것이 특징이다.

RAID1은 일반적으로 OS 디스크 이중화에 사용된다. RAID10은 RAID0과 RAID1을 조합한 구성이다. RAID0은 이중화 없이 HDD에 기록하는 방식이다. 이것은 안정성과 성능을 균형 있게 구성하는 방식이다. RAID10은 복수의 HDD에 병렬로 이중 기록을 하는 방식이다. 이것은 안정성과 성능을 균형 있게 구성하는 방식이다.

RAID5에서는 패리티 연산이 이루어지기 때문에 I/O 성능이 RAID10에 비해 느리다. 하지만 RAID10은 미러링을 하기 때문에 HDD 전체 용량의 1/2 용량밖에 사용할 수 없다. RAID5는 이중화 부분이 적고 '(HDD 수 - 1) / HDD 수'만큼의 용량을 사용할 수 있다. 용량을 중시하는 경우 RAID5를 사용하는 것이 좋다.

HDD가 고장 나면 RAID 구성에 포함된 데이터는 망가지지 않지만, 이중화 구조가 망가진다. 이 이중화 회복을 위해 핫 스페어라고 하는 디스크를 이용한다. 한 대의 HDD가 고장 나면 자동적으로 핫 스페어가 RAID에 포함돼서 이중화 구조를 회복한다. 하지만 핫 스페어가 고갈되고 거기에 HDD까지 파손된 RAID에서는 데이터 손실이 발생하기 때문에 주의가 필요하다.

이와 같이 RAID를 이용해서 이중화를 하더라도 다중 장애 시에는 복구할 수 없는 경우가 있다. 이 때문에 RAID에만 의존하는 것이 아니라 데이터 백업도 반드시 해 두어야 한다.

#### 버스 이중화

저장소 이중화 대상에는 서버와 저장소 사이에 있는 버스도 있다. 예를 들어, RHEL에는 DM-Multipath라는 버스 이중화 기능이 있다.

I/O는 I/O 스케줄러나 드라이버 같은 커널 내부를 통과하지만, DM-Multipath는 I/O 요청을 HBA에 할당해서 페일오버를 실현한다. DM-Multipath는 저장소가 지원하는 경우 액티브-스탠바이로도 사용할 수 있다.

버스 이중화에서 고려해야 할 것은 장애 시의 버스 교체 시간이다. 일반적으로 장애라고 판단하기까지의 HBA 타임아웃 값은 30초 정도로 설정돼 있다. 이것은 불필요한 버스의 페일오버를 발생시키지 않도록 하기 위함이지만, 실제 장애 발생 시에는 페일오버 타임 아웃 시간 동안 저장소 I/O가 정지되기 때문에 처리에 영향을 끼치는 경우에는 값 단축을 검토해야 한다. HBA 드라이버 파라미터로 설정을 변경할 수 있다.

### 웹 서버 이중화

클라이언트의 http 프로토콜 요청을 받는 것은 웹 서버에서 동작하고 있는 웹 서버 프로그램이다. 대표적으로 오픈 소스 웹 서버인 Apache HTTP Server가 있다. 최근 웹 서버는 스레드가 주류이지만, 아파치는 요청 접수시 프로세스 또는 스레드를 선택할 수 있다.

아파치에서는 어느 쪽이든 미리 여러 개를 가동시켜 두어서 클라이언트 요청에 빠르게 대응할 수 있는 구성을 가지고 있다.
여러 개를 가동시켜 두면 프로세스/스레드 중 하나에 장애가 발생해도 다른 프로세스/스레드가 가동되고 있기 때문에 웹 서버의 서비스 전체가 정지되는 일은 없다.

아파치 프로세스/스레드는 httpd다. ps-efL 명령 실행 결과로 프로세스와 스레드의 상태 차이를 알 수 있다.
NLWP는 하나의 프로세스에서 동작하는 스레드 수를 나타낸다.

httpd 프로세스/스레드가 요청을 받지 못하는 상태가 되면 상태 코드로 정보를 알린다. 상태 코드 400번대는 클라이언트 측 에러를 의미하고 500번대는 서버 에러를 의미한다.
웹 서버에 대한 요청은 큐에 쌓이지 않고 바로 에러를 반환한다는 특징이 있다.

#### 서버 이중화

클라리언트가 URL을 입력하면 URL 내의 호스트명을 DNS를 통해 이름 해석을 하고 실제 웹 서버의 IP 주소를 찾는다.

웹 서버를 이중화하는 방법 중 하나는 DNS를 이용해서 하나의 호스트명에 대해 복수의 IP 주소를 반환하는 것이다. 이를 DNS 라운드 로빈이라고 한다. 호스트명에 대해 복수의 IP 주소를 등록해 두는 것으로서 서버 이중화 기법 중 하나다. DNS는 질의에 대해 순서대로 IP 주소를 반환한다. 이 방법은 매우 간단하게 서버를 이중화할 수 이다는 이점이 있지만, 몇 가지 주의 사항이 있다.

1. DNS는 서버 상태를 감시해서 파악하지 않기 때문에 서버가 정지되었어도 그 서버의 주소를 반환한다. 가용성을 중시하는 경우 부적합하다.
2. DNS가 세션 상태를 파악하지 않기 때문에 다음 접속 시에 동일 서버에 접속해야 하는 경우에도 부적합하다. 예를 들어, 웹 서버에 동적 콘텐츠가 있어서 세션 상태를 저장하고 있어야 하는 경우는 DNS 라운드 로빈 방식은 부적합하다.

#### 부하분산 장치를 이용한 웹 서버 이중화

위와 같은 제약 사항 때문에 조금 더 고도화한 이중화 방식이 부하분산 장치(로드 밸런서)다.

세션 유지를 위한 방법은 여러 가지가 있지만, 그 중 하나는 부하분산 장치가 이전에 어느 웹 서버에 요청을 할당했는지를 쿠키에 저장하는 방법이다. 쿠키는 HTTP 프로토콜의 메시지 헤더에 포함되는 요소다. 클라이언트 측에서 쿠키 사용을 허가하면 두 번째 이후 접속부터 HTTP 요청 헤더에 쿠키를 저장해서 접속한다. 부하분산 장치는 이 쿠키를 읽어서 같은 서버에 요청을 할당하는 것이다. 이를 통해 세션 상태를 저장할 수 있다. 부하분산 장치는 임시 대응 관리표로 세션 테이블이라는 것을 만든다. 클라이언트에 요청을 반환할 때는 이 테이블을 참조한다. 세션 상태 저장을 실현하는 기능을 부하분산 장치에서는 '퍼시스턴스' 기능이라고 한다.

퍼시스턴스의 종류:

1. 소스 IP 주소 - 클라이언트 IP 주소를 기반으로 요청을 할당할 웹 서버를 결정한다. (예: 클라이언트 IP 끝자리가 홀수이면 서버 1에 할당)
2. 쿠키 - HTTP 헤더 내에 접속한 웹 서버 정보를 저장한다.
3. URL - URL 구조 내에 접속한 웹 서버 정보를 저장한다.

이런 기술들을 적용하기 전에 각각의 주의 사항 및 고려 사항을 알아 둘 필요가 있다.

출발지 IP 주소를 이용한 퍼시스턴스는 프록시를 경유하면 프록시 서버의 IP 주소가 클라이언트 IP 주소가 돼서 요청이 한쪽으로 몰릴 수 있다.

쿠키를 사용하는 경우는 각 AP 서버 등이 쿠키를 사용할 때 부하분산 장치가 부여한 쿠키를 덮어쓰기하지 않는지 확인해야 한다.

URL에 정보를 심는 경우, URL은 사용자가 직접 편집 가능한 정보이기 때문에 부정 접속에 대한 대책을 검토해 두어야 한다.
일반적으로는 해시 함수를 이용해서 변경한 값을 심는다.
또한, 부하분산 장치의 첫 접속 시 서버 할당 알고리즘에도 몇 가지 방식이 존재한다.

부하분산 장치의 할당 알고리즘
(아래로 갈수록 복잡하다.)

1. 라운드 로빈 - 서버의 IP 주소에 순서대로 요청을 할당
2. 최소 연결 - 세션 수가 가장 적은 서버의 IP 주소에 요청을 할당한다.
3. 응답 시간 - 서버의 CPU 사용률이나 응답 시간 등을 고려해서 가장 부하가 적은 서버의 IP 주소에 요청을 할당.

부하분산 장치는 웹 서버의 가동 상태를 감시할 수 있다.
장애를 감지한 경우는 클라이언트 요청을 동적으로 다른 서버에 할당(페일오버)할 수 있다. 좋아 보이는 기능이지만 한 가지 문제가 있다.

정적 콘텐츠라면 해당되지 않지만, 동적 콘텐츠라면 페일오버와 세션 정보가 사라지기 때문에 세션 상태가 초기화된다. 예를 들어 온라인 쇼핑에서 결제 버튼을 누르기 직전에 오류가 발생하면 과거 입력 내용이 전부 사라지는 것이 있다.

장애 시에 세션 정보를 유지하기 위해서는 페일오버 이외의 다른 구조가 필요하다. 자바에는 세션 정보 보호를 위한 구조가 존재한다.

할당 알고리즘은 간단한 게 좋다. 복잡할수록 높은 부하가 발생한다.

정적 콘텐츠의 경우 세션 수와 CPU 등의 리소스 소비가 비례하기 때문에 단순한 알고리즘인 라운드 로빈이나 최소 연결 방식 등을 주로 사용한다.

부하분산 장치는 고가 장비이기 때문에 소개된 기능 이외에도 'IP:포트' (L4), 혹은 'IP:포트/URL' (L7)을 이용해서 어떤 서버에 리디렉션할지를 정밀하게 설정하는 기능 등도 제공한다.

### AP 서버 이중화

AP 서버 이중화는 두 가지 기능을 이용해서 구현한다.

1. 웹 서버와 같이 부하분산 장치를 이용하거나 AP 서버가 가진 웹 서버 요청 이중화 기능을 이용해서 AP 서버 요청을 분산시킨다.
2. 세션 정보를 이중화한다. 애플리케이션을 실행하는 AP 서버에서는 세션 정보 이중화 기능을 갖추고 있다. 세션 정보는 애플리케이션의 상태를 가리킨다. 애플리케이션 상태를 일시적으로 기억하는 구조라고 보면 된다.

요청의 분산 및 세션 정보 이중화 기능을 사용해서 AP 서버를 이중화할 수 있다. 상용 AP 서버에는 다양한 종류가 있고 그 중 하나가 오라클 웹로직 서버다.

1. AP 서버를 선정해서 요청을 AP 서버에 리디렉션한다.
2. 세션 정보를 작성해서 클러스터 내 다른 서버에 복제한다.
3. 세션 정보가 저장된 서버(기본/보조) 정보를 쿠키의 JSESSIONID에 추가해서 반환한다.

웹로직에는 리디렉션용 플러그인이 있어서 웹 서버에 구현된다.
세션 정보는 접속된 AP 서버를 기본으로 하고 보조 세션을 복사해 둔다. 이 서버 정보는 쿠키에 저장돼서 클라이언트에게 반환된다. 클라이언트가 재접속할 때는 웹 서버에 구현된 리디렉션용 플러그인이 기본 서버를 판별해서 해당하는 AP 서버에 요청을 리디렉션한다.

1. 쿠키 정보로부터 AP 서버 1에 리디렉션해서 실패를 감지한다.
2. 보조 세션이 승격해서 기본 세션이 돼서 리디렉션된다.
3. AP 서버 3에는 세션 정보가 없기 때문에 AP 서버 2에서 복사한다.
4. 세션 정보가 저장돼 있는 서버 정보를 쿠키의 JSESSIONID에 추가해서 반환한다.

쿠키 정보를 가지고 보조 세션 정보에 접속해서 세션이 계속 유지되는 것을 알 수 있다. 세션 정보를 복제하면 이를 위한 메모리나 네트워크 리소스 소비량이 늘어나기 때문에 주의가 필요하다.

#### DB 연결 이중화

AP 서버는 3계층형 시스템의 중간에 위치한다. AP 서버가 DB 서버에 접속하는 경우의 이중화가 존재한다. AP 서버에는 DB 서버에 접속 시에 사용할 연결을 사전에 여러 개 생성해 두는 기능이 있다. 이것을 연결 풀링이라고 한다. 웹로직의 데이터 소스를 설정해서 이용한다. 데이터 소스를 이용하는 장점은 애플리케이션이 DB 서버의 IP나 포트 등을 몰라도 된다는 점이다. 애플리케이션은 데이터 소스명만 알면 된다. 그림에서 GET과 CLOSE는 자바 Connection 객체의 getConnection() 및 Close() 메소드에 해당한다. 애플리케이션은 연결 해제를 하지 않아 연결 생성 및 해제 시에 걸리는 시간이나 리소스 없이 고속으로 처리할 수 있다.

폴링에서 웹로직이 감시에 두 번 실패하면 연결이 끊긴 후 재접속 된다. 연결이 모두 사용 중인 경우 설정 최댓값까지 연결 수가 늘며, 최댓값을 초과한 요구가 오면 일정 시간 대기한다. 파라미터는 '연결 예약 타임아웃'이다. 이 경계 값까지 대기한 후 에러를 반환한다. 아파치와 달리 일단 요구가 큐잉된다.

이와 같은 특징으로부터 연결 풀링의 일반적인 설계 방침은 다음과 같이 정리할 수 있다.

1. 최솟값과 최댓값을 동일하게 설정한다.
   > 연결을 생성하거나 제거할 때 발생하는 오버헤드를 가능한 한 경감시키기 위해서다.
2. 방화벽 유무를 확인해 둔다.
   > 중간에 방화벽이 있다면 오랫동안 사용하지 않은 세션을 자동으로 제거하는 경우가 있기 때문에 방화벽 유무를 확인해두어야 한다. 연결돼 있는 동안 소켓은 Establish 상태이지만, 실제 데이터 교환이 없으면 중간의 네트워크 장비를 해당 연결을 대기 상태라고 간주한다. 특히, 방화벽에서는 보안상의 이유로 장시간 대기 상태인 연결을 절단하는 기능이 있다. 때문에 의도하지 않은 절단이 발생할 수 있어 정기적으로 폴링하는 등 미리 대책을 마련해 두어야 한다.

### DB 서버 이중화

#### 서버 이중화(액티브-스탠바이)

DB 서버 이중화 방법으로는 수년 전부터 주류가 되고 있는 액티브-스탠바이형의 클러스터(Cluster) 구성이 있다. 클러스터 구성은 하드웨어로도 구현할 수 있지만, 일반적으로 클러스터 소프트웨어를 이용한다.

클러스터 구성은 HA(High Availability, 고가용성) 구성이라고 부른다. 클러스터의 노드나 서비스 관계는 마스터-워커 개념을 기반으로 하고 있다. 서버가 정상 동작하는지 확인하기 위한 구조로 '하트비트'(상호 간에 정상 동작하는지 확인하기 위해 정보를 공유한다.)나 '투표 장치' 같은 기능이 존재한다. 서버가 두 대가 있다면 두 대의 서버가 한 가지 역할을 담당한다.

페일오버 구조

1. 장애를 감지한다.
2. 각 노드의 상태가 투표 장치에 기록된다.
3. 장애를 감지하면 서비스를 정지하거나 OS를 재시작한다.
4. 액티브 측이 정지된 것을 인식하고 스탠바이가 서비스를 시작한다.

클러스터 소프트웨어는 등록된 서비스가 정상 동작하고 있는지 정기적으로 확인한다. 이상이 발생하면 서비스를 정지하고 대기하고 있던 스탠바이 측 서비스를 시작해서 서비스를 유지시킨다. 일반적으로 십여 분 정도의 정지 시간으로 서비스를 재개할 수 있다.

클러스터 소프트웨어는 하트비트를 이용해서 상호 간의 상태를 확인하기 때문에 하트비트를 통해 상태를 인식할 수 없게 되면 클러스터 소프트웨어는 페일오버 실시 여부를 판단할 수 없게 된다. 이런 상태를 '스플릿 브레인'이라고 한다.

스플릿 브레인을 해결하는 방법은 다음과 같다.

1. 액티브와 스탠바이가 투표를(상태나 통신 가능한 노드 기록) 한다.
2. 먼저 기록한 쪽이 액티브가 된다.

3노드 이상의 클러스터에서는 상호 인식한 수가 많은 노드가 살아남고 소수파는 정지된다. 이처럼 투표 장치는 하트비트 기능을 보완하는 역할을 한다.

클러스터 구성용 서비스?

클러스터 소프트웨어를 이용한 액티브-스탠바이 구성은 서비스를 병렬로 실행할 수 없고 데이터 일관성을 중시하는 서비스/시스템에 적합하다. 예를 들면 데이터베이스, 파일 서버, 잡 관리 시스템 등이다. 따라서 데이터 갱신이 거의 없어서 데이터 일관성이 중시되지 않고, 서비스 병렬 실행이 가능한 웹 서버나 AP 서버에서는 클러스터 소프트웨어를 사용하지 않는다.

클러스터 S/W 이용 시 주의할 점은 클러스터 S/W도 OS에서 실행되는 S/W이기 때문에 오동작할 가능성이 있다는 것이다. 스플릿 브레인 대책처럼 클러스터 소프트웨어도 자신에게 발생한 장애에 대처할 수 있지만, 100% 신뢰할 수는 없다. 이 때문에 반드시 중요한 데이터는 백업해 두고, 이중 안전 장치가 필요한 경우는 원격 복제 기능 등을 이용한다.

#### 서버 이중화(액티브-액티브)

DB 서버의 데이터 참조, 갱신 부분은 시스템의 병목 지점이 되기 쉬워서 항상 높은 확장성이 요구된다.
이 때문에 확장 가능한 다양한 기술들이 생겨나고 있다.

shared everything - 디스크, 데이터를 모든 노드가 공유한다. 장애가 발생해도 다른 노드로 쉽게 처리를 계속할 수 있다.
shared nothing - 각 노드별로 디스크를 가지고 있어서 데이터가 분산된다. 노드를 배치하기 쉽다.

대량의 데이터를 검색하는 경우는 데이터가 분산돼 있기 때문에 쉐어드 낫씽형 쪽이 유리하다. 작은 트랜잭션이 대량으로 발생하는 경우도 노드 추가를 통해 쉽게 확장할 수 있기 때문에 쉐어드 낫씽형이 유리하다. 하지만 확장 시에는 데이터 재배치를 검토 및 설계해야 하기 때문에 확장이 쉽지 않다. 또한, 갱신 시에 데이터 분산 위치를 검토하기 때문에 전반적인 갱신 처리가 느려지는 경향이 있다.

쉐어드 에브리씽형에서는 다른 노드에 있는 메모리 데이터의 일치성을 확인할 필요가 있기 때문에 노드 수를 늘려도 확장이 쉽지 않다. 또한, 모든 노드가 같은 데이터에 액세스할 수 있지만, 이 경우 서로 배타적 제어나 데이터 경합을 하기 때문에 처리 속도가 저하된다.

쉐어드 에브리씽형이 어떤 노드에서건 같은 데이터에 액세스할 수 있기 때문에 가용성 면에서는 유리하지만, 쉐어드 낫씽형에서도 데이터 복제 기능을 이용해서 안정성을 고려한 데이터베이스를 사용할 수 있다.

시스템 특성에 따라 해당 시스템의 병목 현상이나 장애 시 영향 등을 고려하는 것이 중요하다.
클라우드상에서 구현할 때는 쉐어드 낫씽형이 압도적으로 많다. 쉐어드 에브리씽형에서는 공유 디스크가 필수로, 지리적으로 블랙박스화돼 있는 클라우드에선 구성이 어렵기 때문이다. 만약 클라우드상에서 구현한다면 물리 서버를 완전히 점유하는 서비스를 이용해야 한다. 최신 클러스터 소프트웨어에선 클라우드에 대응하기 위해 쉐어드 낫씽으로 변경되고 있다.

트렁크 포트를 사용하려면 VLAN 데이터를 식별할 수 있도록 설정해야 한다.

### 네트워크 장비 이중화

#### L2 스위치 이중화

두 개의 서버를 스위치 두 개의 포트에 교차해서 꽂으면 스위치 장애에 대비할 수 있다. 스위치를 크로스 케이블 등으로 연결하면 서로 다른 스위치 간 통신이 가능하다. 이것을 캐스케이드라고 한다. 하지만 최근에는 하나의 스위치를 하나의 네트워크에서만 이용하는 것이 아니라 복수의 네트워크에 연결하는 경우도 있다. 이런 경우에는 스위치가 복수의 VLAN을 설정한다.

스위치 간에 VLAN 통신을 할 때는 각각의 VLAN을 연결해야 한다. 하지만 트렁크 포트라는 포트를 이용하면 포트를 복수의 VLAN에 소속시킬 수 있다. 이렇게 트렁크 포트를 사용하는 것이 주류가 되고 있다.

하지만 트렁크 포트에 장애가 발생하면 스위치 전체에 장애가 발생한다. 네트워크 이중화 방식 중 '링크 집합'이 있다. 서버나 NAS 등에서는 여러 포트를 합쳐서 하나의 이더넷 포트로 이용할 수 있다. 리눅스의 본딩 기능에서도 가능하다. 이것을 '링크 집합(Link Aggregation)'이라고 한다. 링크 집합에서는 보통은 양쪽 포트를 사용하지만, 포트 장애 시에는 해제해서 처리를 계속 할 수 있다. 또한, 양쪽 포트를 사용함으로써 대역이 배로 증가하기 때문에 병목 현상 해결책으로도 이용할 수 있다. 일반적으로 최대 4선 정도까지 묶을 수 있다.

서버 측 포트를 묶은 경우는 꽂고 있는 스위치 쪽 포트도 같은 방식으로 묶어야 한다. 시스코 스위치에서는 이 기능을 '이더 채널'이라고 부른다. 이더 채널에서는 트렁크 포트도 묶을 수 있다. 트렁크 포트는 이 기능을 이용해서 여러 포트를 묶어 사용함으로써 트렁크 포트 통신이 병목 지점이 되는 것을 방지하고 안정성을 높일 수 있다.

여러 개로 묶은 네트워크 포트를 사용해서 부하분산하는 방식에는 여러 가지가 있다.
예를 들어, 본딩에는 라운드 로빈 방식이 있다. 이더 채널에서는 송수신이나 목적지 MAC 주소에 따라 어떤 포트를 사용할지 결정하는 방식이 있다. 목적지 MAC 주소에 따라 부하분산을 하는 경우, 목적지 서버가 한 대라면 하나의 포트에 통신이 집중되기 때문에 설정 시에 이를 고려해야 한다.

#### L3 스위치 이중화

L3 스위치 이중화는 기본적으로 액티브-스탠바이다. 최근 시스코 등에서 액티브-스탠바이에 이용할 수 있는 Virtual Switching System(VSS)과 같은 기능도 제공하고 있다.

웹 시스템에서는 게이트웨이가 다운되면 시스템 서비스가 거의 모두 정지된다고 해도 과언이 아니다. 따라서 L3 스위치 이중화가 매우 중요하다. 구체적인 구현 방법으로는 L3 스위치의 액티브-스탠바이를 실현하는 프로토콜인 Virtual Router Redundancy Protocol (VRRP)라는 것이 있다.

마찬가지로 정기적인 하트비트(advertisement)를 보내서 생존 감시를 한다.
보조 장비가 애드버타이즈먼트를 일정 시간 수신하지 못하면 마스터 라우터 역할을 인계한다.

#### 네트워크 토폴로지

복수의 경로가 존재하는 네트워크 구성을 '루프(Loop)'라고 한다.
하지만 경로가 다수 존재하면 안정성 측면에서 좋지 않다. 이 모순을 해결하기 위한 수단으로 스패닝 트리 프로토콜(STP)이라는 것을 이용할 수 있다.

STP를 이용하면 논리적으로 포트를 절단할 수 있다. 절단할 포트는 STP 계산 알고리즘에 의해 정해지지만, 스위치 설정을 통해 절단 대상을 제어할 수도 있다.

장애 시에는 STP에 의한 재계산이 이루어지며, 논리적으로 절단돼 있는 포트를 개통해서 통신이 가능해진다.

네트워크 구성에는 몇 가지 패턴이 존재한다. 사다리형, 십자형 등이 있는데 모두 STP를 이용해서 블록을 한다. 십자형은 1루프에 관여하는 스위치 수가 세 개이며, 사다리형은 네 개다. 최근에는 십자형이 주로 사용된다.

### 사이트 이중화

대규모 재해가 발생하면 데이터 센터 전체가 가동되지 않을 수도 있다. 이런 재해에 대한 대책으로 원격지 데이터 센터와 연계하는 기술이 있다. 글로벌 서버 부하분산(GSLB)이라는 구조다. 글로벌 부하분산 장치 간에는 항상 사이트 정보가 연계된다.
DNS가 반환하는 IP 주소를 동적으로 변경한다. 이와 같은 기능을 이용해서 사이트 장애에 대비할 수 있다. 데이터 관련 사이트 이중화는 저장소 측 복제를 이용한다.

이런 재해 대책을 목적으로한 데이터 전송 기술을 통틀어서 '재해 복구'라고 부르는 경우도 있다.

원격지에 데이터를 전송할 때 중요한 것은 동기/비동기 여부다. 데이터를 완전히 지키고 싶을 때는 데이터가 원격지에도 기록될 필요가 있기 때문에 동기화시킨다.
데이터를 원격지와 동기화시킬 때 오버헤드가 많이 걸려서 응답 속도가 느려지는 단점이 있다. 비동기로 하면 응답은 좋지만 데이터를 완전하게 지킬 수 없다.

사이트가 떨어져 있을 때는 비동기로 해서 어느 정도의 데이터 손실을 감수하고 있는 구성이 일반적이긴 하다.

### 감시

감시 기능은 시스템 컴포넌트가 정상으로 동적하는지 확인하는 기능이다. 시스템 서비스를 안전하게 지속하기 위해서는 감시는 필수적이다.

감시에는 대표적으로 다음과 같은 것이 있다.

1. 생존 감시
2. 로그(에러) 감시
3. 성능 감시

이외에도 하드웨어 자신이 하드웨어 고장을 감시하거나 클러스터 소프트웨어가 실시하는 각 컴포넌트 감시 등 다양한 감시가 있다.

감시에서 중요한 것은 어떤 목적으로 감시 기능이 필요한지, 특정 컴포넌트에 감시가 너무 중복돼 있지 않은지 등을 고려하는 것이다. 감시 대상에 대해서 극단적으로 얘기하자면, 모든 프로세스를 감시해서 로그가 하나라도 출력되면 그것을 감지할 수 있도록 설정해야 한다. 하지만 실제로 경고가 발생해도 어떤 식으로 대처하면 좋을지, 경고 발생 후 행동으로 연결되지 않는다면 의미가 없다. 또한, 감시 수가 너무 많으면 경고가 전달돼도 무시하게 된다. 이 때문에 감시의 본질적인 의미가 사라질 수 있다.

#### 생존 감시

대표적인 감시 중 하나다. ping 명령을 정기적으로 실행해서 서버 인터페이스에 대한 통신을 확인하는 감시를 할 수 있다. 이를 생존 감시하고 하며 ping 감시라고도 부른다. ping을 이용한 감시는 구현이 매우 쉬워서 어떤 시스템이라도 구현할 수 있다.

대부분의 감시 툴은 프로세스가 정상 동작하는지를 OS의 ps 명령을 이용해서 확인한다. ping처럼 매우 간단한 감시다.

프로세스 감시는 실행 중인 프로세스 모두를 감시하는 것이 아니라 중요한 것만 추려서 감시하는 것이 좋다. 해당 프로세스에 장애가 발생하면 어떤 영향이 있는지, 어떤 대책이 필요한지와 같은 관점에서 생각하는 게 중요하다. 예를 들어, 리눅스 환경에서는 기본으로 실행되는 cupsd(인쇄 서버) 프로세스가 다운되어도 인쇄 서버를 구성하고 있지 않는 한 아무런 문제가 없다. 폴링을 통해 정기적으로 ps 명령을 실행하고 프로세스가 있으면 정상임을 알 수 있다.

#### 로그 감시

OS나 미들웨어가 출력하는 로그 파일에는 시스템 유지를 위한 중요 정보가 포함돼 있다. 미들웨어 오류나 영역 고갈 등 생존 감시로는 알 수 없는 정보가 로그 파일로 출력된다. 또한, 장애 원인 분석에도 도움이 된다.

감시 프로세스가 로그 파일을 정기적으로 확인한다. 로그 파일에 새로운 출력이 있으면 패턴에 해당하는 내용이 있는지 확인한다. 해당하는 경우 통지를 보낸다.
로그 감시 프로세스는 중요하다고 인지하고 있는 로그 출력문을 미리 저장해 두고 있다가 실제 로그 파일이 출력되는 내용을 저장해 둔 것과 비교한다. 일치하는 내용이 있으면 문제의 중요도에 맞추어 감시 서버에게 보고한다. 통지 방법은 syslog, SNMP, 메일 등 몇 가지가 있다.

대부분의 미들웨어 로그는 [alert], [error], [notice] 등의 문자열을 붙여서 출력한다. 키워드 패턴에는 시스템 유지에 중요한 에러 전체를 기술할 수 없기 때문에 위와 같은 키워드를 등록해 둔다. 오라클 DB에서는 경고 로그라 불리는 로그 파일에 'ORA-' 등의 키워드를 붙여서 로그를 출력한다. 따라서 조회 패턴에는 'ORA-' 키워드를 등록해 두어야 한다.

#### 성능 감시

성능 감시는 위의 두 가지 감시보다 감시 내용이 복잡하다. 디스크 사용률이나 메모리 사용 현황, 디스크 고갈 등의 리소스 상태 파악과 네트워크 액세스 지연, 디스크 액세스 시간 등의 응답 상태를 파악하는 것이다. df 명령 등의 OS 명령을 정기적으로 실행하거나 vmstat 명령이나 sar 명령 등의 통계 정보를 취득해서 상황을 통계적으로 판단하는 등 다양한 방식이 가능하다.

| 감시 대상              | 감시 내용                                                        |
| ---------------------- | ---------------------------------------------------------------- |
| CPU                    | CPU 사용률, CPU 대기 행렬                                        |
| 메모리                 | 빈 메모리 양                                                     |
| DISK                   | 남은 용량, 디스크 액세스 시간                                    |
| 네트워크               | I/F 인바운드/아웃바운드 대역 사용률, 패킷 손실                   |
| HTTP(웹 서버 고유)     | HTTP 요청의 응답 시간, 초당 HTTP 요청 처리 수, 초당 HTTP 세션 수 |
| JAVA(AP 서버 고유)     | 메모리 힙 크기, 가비지 컬렉션 횟수                               |
| DATABASE(DB 서버 고유) | 영역의 남은 용량, 캐시 사용률, SQL 응답                          |

성능 감시 항목 선별 및 경계 값 설정, 이상 값 분석은 각각의 미들웨어와 시스템 아키텍처를 고려해서 실시해야 한다.

#### SNMP

실제 시스템에서 감시 기능의 구현 방법이나 통지 수단에는 여러가지가 있다. 통합 감시 툴로 상용 제품이 다수 존재하며, 히타치의 JP1, IBM의 티볼리 등이 대표적이다. 최근에는 오픈 소스인 Zbbix나 Nagios와 같은 툴로 상용 환경을 감시하는 경우도 늘고 있다. 기능도 다양하지만, 이 기능들은 감시 전용 프로토콜을 기반으로 하고 있다. 이를 SNMP라고 부른다. SNMP를 이용해서 감시할 수 있는 주요 내용은 다음과 같다. 확장도 가능해서 다른 여러 사항도 감시할 수 있다.

- 네트워크 장비나 서버 가동 상태
- 서비스 가동 상태
- 시스템 리소스(시스템 성능)
- 네트워크 트래픽

SNMP는 네트워크 장비와 서버를 일괄 감시해서 관리할 수 있는 것이 특징이다.

SNMP 구성에는 감시 서버에 매니저가 있으며 감시 대상 서버 및 네트워크 장비에 에이전트가 존재한다.

감시 경로에는 매니저가 정기적으로 질의하는 폴링과 이상 발생 시에 에이전트가 통지하는 트랩 등 두 가지가 있다. 폴링은 주로 리소스 상태를 감시할 때 이용한다. 통신은 구체적으로는 UDP 프로토콜로 161, 162번 포트를 이용한다.

SNMP의 주요 특징으로 MIB(관리 정보 기반)라는 것이 있다. MIB는 감시 정의 모델이다. 에이전트는 MIB에 규정된 정보를 수집해서 매니저에게 통지한다. 매니저와 에이전트는 상호 대화를 하기 때문에 같은 MIB를 소유한다.

매니저와 에이전트가 MIB를 바탕으로 정보를 수집해서 OID를 통해 정보를 공유한다. OID는 MIB에 정의되어 있는 일종의 식별자다.

MIB는 데이터베이스 형태와 비슷하다. MIB는 트리 구조로 만들어져 있다.
각 노드는 번호를 갖고 있고, 정의되는 모델은 루트에서부터 순서대로 숫자를 연결한다. (예: 1.3.6.1.2.1.2)

이와 같이 SNMP는 감시에 특화된 프로토콜이다. SNMP의 주의점은 SNMP 트랩은 원칙적으로 재전송하지 않으며, 장애로 트랩을 수신하지 못한 경우에는 그 트랩을 잃어버린다는 것이다. 모든 통지를 수신해야 한다면 메일을 사용할 수 있다.
메일은 축적 및 저장이 용이하지만, MIB 같은 범용적인 정의가 없으며, 단지 메일함에 저장되는 것이 전부로 종합 감시 콘솔 등에 표시되지 않는다. 따라서 운영 방식을 고려해서 적용하는 것이 필요하다.

#### 콘텐츠 감시

콘텐츠 감시는 웹 화면이 정상적으로 보여지는지 확인하기 위한 웹 시스템 특유의 감시다. 클라이언트에게 정상적으로 응답을 반환하면 웹 시스템이 정상 가동하고 있다고 볼 수 있으므로, 콘텐츠 감시는 전체적으로 중요한 감시이다. 일반적으로 콘텐츠 감시는 부하분산 장치가 담당한다.

부하분산 장치에 감시 대상 URL을 등록해 둔다. HTTP의 GET 요청을 해서 정상적으로 응답이 있으면 해당 웹 서버 또는 웹 서버 + AP 서버가 정상 가동되고 있다고 판단한다. 만약 응답이 오지 않으면 장애가 있다고 판단해서 부하분산 장치가 해당 웹 서버에는 요청을 할당하지 않는다.

감시 구조에는 폴링 이론이 많이 사용되고 있다는 것을 알 수 있다. 그만큼 폴링 개념이 중요하다고 할 수 있다.

### 백업

장애 대책을 생각할 때 이중화 등을 도입해서 서비스를 지속하는 것도 중요하지만, 만일의 경우에 대비해서 백업을 만들어 두는 것도 매우 중요하다. 이중화와 크게 다른 점은 데이터를 복제해서 별도 장소에 보관한다는 것이다. 이 때문에 '복원'이나 '복구'를 이용해서 데이터를 원래 장소로 되돌리는 과정이 필요하다. 백업은 데이터를 복제해 두기만 한다고 괜찮은 것이 아니다. 백업 취득 빈도나 시점은 복원을 고려해서 계획을 세워야 한다. 구체적으로는 다음과 같이 복구 지표를 정해서 백업을 설계한다.

1.  RTO(Recovery Time Objective): 복구 목표 시간
2.  RPO(Recovery Point Objective): 복구 기준 시점

1번은 수 시간 내에 복구하지 않으면 업무에 큰 영향을 끼치는 시스템이나, 수 일 동안 업무 대체가 가능해서 천천히 복구해도 되는 것 등 시스템에 따라 백업 요건이 달라진다. 당연히 RTO가 짧을수록 설계 난이도가 높고 백업 시스템 가격도 비싸진다.

2번은 데이터를 최신 상태로 만들지 않으면 곤란할 것 같지만, 일괄 처리가 주류인 시스템에서는 일괄 처리 실행 직후까지 되돌리면 업무에 문제가 없는 경우도 있다.

RPO: 데이터를 어느 정도의 과거 시점까지 되돌릴 것인가?
RTO: 복구 작업에 어느 정도 시간이 걸리는가?

시스템에서 백업해야 하는 대상은 다음 두 가지다.

1.  시스템 백업(OS나 미들웨어 등의 백업)
2.  데이터 백업(데이터베이스나 사용자 파일)

#### 시스템 백업

시스템 백업은 OS나 미들웨어 등 일반 서버의 로컬 디스크 영역을 백업하는 것이다.

OS나 미들웨어는 한 번 설치해서 설정이 끝난 후에는 많은 변경이 발생하지 않는다. 이 때문에 백업 빈도는 데이터에 비해 적은 편이다.

시스템 백업은 다음과 같은 시점에서 실시한다.

- 초기 구축 후
- 일괄 처리 적용 시
- 대규모 구성 변경 시

취득 방법에는 다음과 같은 것이 있다.

- OS 명령(tar, dump 등)
- 백업 소프트웨어

취득 매체로는 테이프나 최근에 사용되고 있는 DVD 등이 있으며, 가상 환경에서는 파일 등이 사용된다.

백업은 압축 기능이 유효한 처리다. 왜냐하면 백업은 데이터 갱신이 발생하지 않고, 평상시에는 사용하지 않는 데이터로 가능한 작은 크기로 저장해 두고 싶은 데이터이기 때문이다. RTO 시간 내에 압축 처리를 끝낼 수 있다면 압축을 사용한다.

시스템 백업 취득 시의 유의점은 서버의 서비스를 정지할 필요가 있다는 것이다. 가동 중인 서비스는 백업할 수 없다. 미들웨어가 가동 중인 상태에서 백업을 취득하면 임시 파일이나 프로세스 가동 정보도 취득하게 돼서 복구 시에 정상 가동되지 않는 경우가 있기 때문이다. 시스템 백업은 안전을 고려해서 계획된 시스템 정지 일정에 맞추어 실시해야 한다.

#### 데이터 백업

데이터 백업은 시스템 백업과 달리 매일 변경되는 데이터가 손실되지 않도록 하는 것으로, 취득 빈도가 높다. 이 때문에 정지하기 힘든 시스템은 서비스가 가동 중인 상태라도 백업이 가능한 구조가 필요하다. 하지만 그런 시스템일수록 확실하게 데이터 일치성을 보장해야 하며, 특히 데이터베이스 시스템에 있어서는 일치성 보장 기능이 필수다. 이와 같은 사정 때문에 데이터베이스 백업은 데이터 자체와 데이터 갱신 내역이 기록돼 있는 저널을 모두 취득하도록 하고 있다.

데이터는 기본적으로 서비스를 정지한 후 변경이 발생하지 않은 상태에서 취득한다. 오라클 DB 등 일부 데이터베이스에서는 서비스가 가동 중인 경우라도 데이터 백업을 할 수 있긴 하다. 저널 로그는 트랜잭션 정보가 기록돼 있으며, 갱신이 발생하지 않는다. 따라서 임의의 시점에 서비스를 중지하지 않고도 백업을 할 수 있다.

데이터베이스가 망가진 경우라면 일관성 있는 데이터와 저널 로그 양쪽을 복원한다. 그리고 일관성 있는 데이터에 대해 저널 로그를 바탕으로 다시 트랜잭션을 실행해서 복구한다. 또한, 백업되지 않은 저널 로그가 장애 발생 직전까지 존재한다면 최신 데이터로 복구할 수 있다.

백업 취득 방법은 다양하다. 데이터베이스 전용 명령어나 OS 명령을 이용할 수도 있고, 저장소 차분 데이터 블록 복사 기능을 이용하는 경우도 있다. 데이터를 백업한 것도 시스템 백업과 마찬가지로 압축하는 것이 좋다.
